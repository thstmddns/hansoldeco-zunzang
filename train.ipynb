{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team 전장갓겜 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OS: Ubuntu 20.04 LTS  \n",
    "Pytorch: 2.0.1  \n",
    "CUDA: 11.7  \n",
    "cuDNN: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas sentence_transformers transformers==4.37.1 tqdm pyarrow wandb spacy matplotlib\n",
    "!pip install bitsandbytes==0.41.1 accelerate==0.21.0 appdirs loralib black black[jupyter] datasets fire sentencepiece scipy numpy scikit-learn\n",
    "!pip install git+https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import transformers\n",
    "import datasets\n",
    "import transformers\n",
    "import sys\n",
    "\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix SEED\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'auto' \n",
    "base_LLM_model = \"mncai/llama2-13b-dpo-v3\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_LLM_model,\n",
    "    load_in_8bit=True, # LoRA\n",
    "    #load_in_4bit=True, # Quantization Load\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_LLM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check special token\n",
    "bos = tokenizer.bos_token_id \n",
    "eos = tokenizer.eos_token_id \n",
    "pad = tokenizer.pad_token_id\n",
    "tokenizer.padding_side = \"right\" \n",
    "\n",
    "if (pad == None) or (pad == eos):\n",
    "    tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "\n",
    "# 데이터셋과 훈련 횟수와 관련된 하이퍼 파라미터\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "micro_batch = 1\n",
    "gradient_accumulation_steps = batch_size // micro_batch\n",
    "\n",
    "# 훈련 방법에 대한 하이퍼 파라미터\n",
    "cutoff_len = 4096\n",
    "lr_scheduler = 'cosine'\n",
    "warmup_ratio = 0.06 # warmup_steps = 100\n",
    "learning_rate = 4e-4\n",
    "optimizer = 'adamw_torch'\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# LoRA config\n",
    "lora_r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# Tokenizer에서 나오는 input값 설정 옵션\n",
    "train_on_inputs = False\n",
    "add_eos_token = False\n",
    "\n",
    "# Others\n",
    "resume_from_checkpoint = False \n",
    "output_dir = './custom_LLM_llama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train_data_JJGG.csv')\n",
    "formatted_data = []\n",
    "for _, row in tqdm(data.iterrows()):\n",
    "    for q_col in ['질문_1', '질문_2']:\n",
    "        for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "            formatted_data.append({\n",
    "            'input':'',\n",
    "            'instruction': row[q_col],\n",
    "                'data_source': '',\n",
    "                'output': row[a_col]\n",
    "            })\n",
    "formatted_df = pd.DataFrame(formatted_data)\n",
    "formatted_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dacon_dataset = Dataset.from_pandas(formatted_df)\n",
    "\n",
    "# Instruction tuning을 위한 template 작성\n",
    "instruct_template = {\n",
    "    \"prompt_input\": \"아래는 작업을 설명하는 지침과 추가 입력을 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 입력:\\n{input}\\n\\n### 답변:\\n\",\n",
    "    \"prompt_no_input\" : \"아래는 도배 분야와 관련된 질문입니다. 질문에 적절한 답변을 간단하게 작성해주세요. \\n\\n### 지침:\\n{instruction}\\n\\n### 답변:\\n\",\n",
    "    \"response_split\": \"### 답변:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 불러오는 클래스\n",
    "class Prompter(object):\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        self.template = instruct_template\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "\n",
    "        if input: # input text가 있다면\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "prompter = Prompter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token generation 함수\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"])\n",
    "\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"])\n",
    "\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token)\n",
    "\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 셋 만들기\n",
    "val_data = None\n",
    "train_data = dacon_dataset.shuffle() # random\n",
    "train_data = train_data.map(generate_and_tokenize_prompt)\n",
    "\n",
    "# LoRA config 정의\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, config) # Applying LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_from_checkpoint:\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # All checkpoint\n",
    "\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model\n",
    "        resume_from_checkpoint = (\n",
    "            True\n",
    "        ) \n",
    "\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class 정의\n",
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size = micro_batch,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            max_grad_norm = max_grad_norm,\n",
    "            save_steps = 30, \n",
    "            lr_scheduler_type=lr_scheduler,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=False,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length = False,\n",
    "            report_to=\"none\"\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters() \n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save_pretrained(output_dir)\n",
    "model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "torch.save({}, model_path)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# 훈련된 LoRA layer와 base LLM 병합(merge)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_LLM_model,\n",
    "    return_dict = True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, output_dir, device)\n",
    "model = model.merge_and_unload() \n",
    "\n",
    "final_save_folder = './custom_LLM_llama_final'\n",
    "model.save_pretrained(final_save_folder)\n",
    "tokenizer.save_pretrained(final_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    base_model = \"mncai/agiin-13.6B-v0.1\"\n",
    "    epochs = 5\n",
    "    output_directory = './custom_LLM_agiin'\n",
    "    final_output_directory = './custom_LLM_agiin_final'\n",
    "    train_keyword_csv = 'train_data_JJGG.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'auto' \n",
    "base_LLM_model = CFG.base_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_LLM_model,\n",
    "    load_in_8bit=True, # LoRA\n",
    "    #load_in_4bit=True, # Quantization Load\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_LLM_model)\n",
    "tokenizer.padding_side = \"right\"\n",
    "if (pad == None) or (pad == eos):\n",
    "    tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "\n",
    "# 데이터셋과 훈련 횟수와 관련된 하이퍼 파라미터\n",
    "batch_size = 16\n",
    "num_epochs = CFG.epochs\n",
    "micro_batch = 1\n",
    "gradient_accumulation_steps = batch_size // micro_batch\n",
    "\n",
    "# 훈련 방법에 대한 하이퍼 파라미터\n",
    "cutoff_len = 4096\n",
    "lr_scheduler = 'cosine'\n",
    "warmup_ratio = 0.06 # warmup_steps = 100\n",
    "learning_rate = 4e-4\n",
    "optimizer = 'adamw_torch'\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# LoRA config\n",
    "lora_r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# lora_target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# llama\n",
    "lora_target_modules=[\n",
    "\"q_proj\",\n",
    "\"up_proj\",\n",
    "\"o_proj\",\n",
    "\"k_proj\",\n",
    "\"down_proj\",\n",
    "\"gate_proj\",\n",
    "\"v_proj\"]\n",
    "\n",
    "# KuLLM\n",
    "# target_modules = [\"query_key_value\"]\n",
    "\n",
    "# Tokenizer에서 나오는 input값 설정 옵션\n",
    "train_on_inputs = False\n",
    "add_eos_token = False\n",
    "\n",
    "# Others\n",
    "resume_from_checkpoint = False\n",
    "output_dir = CFG.output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "data = pd.read_csv(CFG.train_keyword_csv)\n",
    "\n",
    "formatted_data = []\n",
    "for _, row in tqdm(data.iterrows()):\n",
    "    for q_col in ['질문_1', '질문_2']:\n",
    "        for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "            formatted_data.append({\n",
    "            'input':'',\n",
    "            'instruction': row[q_col],\n",
    "                'data_source': '',\n",
    "                'output': row[a_col]\n",
    "            })\n",
    "formatted_df = pd.DataFrame(formatted_data)\n",
    "formatted_df.shape\n",
    "\n",
    "from datasets import Dataset\n",
    "dacon_dataset = Dataset.from_pandas(formatted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_template = {\n",
    "    \"prompt_input\": \"아래는 작업을 설명하는 지침과 추가 입력을 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 입력:\\n{input}\\n\\n### 답변:\\n\",\n",
    "    \"prompt_no_input\" : \"아래는 도배 분야와 관련된 질문입니다. 질문에 적절한 답변을 간단하게 작성해주세요. \\n\\n### 지침:\\n{instruction}\\n\\n### 답변:\\n\",\n",
    "    \"response_split\": \"### 답변:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompter()\n",
    "val_data = None\n",
    "train_data = dacon_dataset.shuffle() \n",
    "train_data = train_data.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config 정의\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")\n",
    "\n",
    "# Model with LoRA\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_from_checkpoint:\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # All checkpoint\n",
    "\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model\n",
    "        resume_from_checkpoint = (\n",
    "            True\n",
    "        ) \n",
    "\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class 정의\n",
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size = micro_batch,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            max_grad_norm = max_grad_norm,\n",
    "            save_steps = 30, \n",
    "            lr_scheduler_type=lr_scheduler,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=False,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length = False,\n",
    "            report_to=\"none\"\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters() \n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "torch.save({}, model_path)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 LoRA layer와 base LLM 병합(merge)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_LLM_model,\n",
    "    return_dict = True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, output_dir, device)\n",
    "model = model.merge_and_unload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "final_save_folder = CFG.final_output_directory\n",
    "\n",
    "model.save_pretrained(final_save_folder)\n",
    "tokenizer.save_pretrained(final_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtorch_kernel",
   "language": "python",
   "name": "newtorch"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
